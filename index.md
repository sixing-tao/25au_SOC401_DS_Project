---
layout: default
---

<img src="{{ site.url }}{{ site.baseurl }}/assets/img/eScience.png">

# AI Privacy in Large Language Model Training

<img src="{{ site.url }}{{ site.baseurl }}/assets/img/Gemini_Generated_Image_vy26rjvy26rjvy26.png" alt="AI Privacy in Large Language Model Training" style="width: 100%; max-width: 800px; height: auto; margin: 20px 0;">

## Policy-relevant Issue

The rapid expansion of large language models (LLMs) has intensified debates about privacy, consent, and accountability in AI development. As current systems rely on massive datasets scraped from the internet, questions arise about how personal, copyrighted, or sensitive content is collected and used. These concerns highlight the growing need for clear data-governance frameworks that balance innovation with individual rights and public trust.

<iframe width="600" height="371" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vSD0pPL67epFjFuA77G0zX213P1QwzLQ38zns_Ysmh4d-vfVxqVzYjyo_YlrwS7ercGlc5NMM6JmZ4Q/pubchart?oid=1561855552&amp;format=interactive"></iframe>

## Stakeholders

**AI companies:** Build and refine large language models using vast datasets collected from the internet, often containing personal or copyrighted content. While they emphasize the need for such large-scale data to improve model accuracy and competitiveness, they face increasing public and regulatory scrutiny over transparency and accountability.

**Content creators and internet users:** Generate online materials that may be incorporated into training datasets without explicit permission or compensation, raising concerns about ownership, privacy, and data misuse.

**Policymakers and regulators:** Define and enforce data-privacy, intellectual-property, and transparency standards that govern AI development and corporate accountability.

**Digital-rights organizations:** Advocate for stronger oversight and ethical governance of how training data—especially personal or copyrighted material—is collected, stored, and used.

**End users of AI technologies:** Benefit from enhanced AI capabilities in education, accessibility, and productivity, but their trust and satisfaction may decline if they perceive that their data is being mishandled or insufficiently protected.

## Problem Statement

As large language models rapidly expand across industries, differences in how companies collect, disclose, and govern training data have heightened concerns about privacy, consent, and accountability. Many AI firms rely on vast web-scraped datasets that may include personal or copyrighted content, yet often provide little clarity about what data is used or how individuals' information is protected. At the same time, governments differ in how strictly they regulate data use and disclosure, leading to uneven standards of privacy protection. These disparities raise critical questions about how corporate practices and governmental data-governance frameworks jointly shape public trust and satisfaction with AI technologies. Policymakers face the challenge of fostering innovation while ensuring that individuals' rights and personal data are safeguarded in the development of emerging technologies.

## Research Question

**In what ways do corporate transparency and governmental data-privacy regulations shape public trust and satisfaction with AI technologies?**
