---
---

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Governing AI Trust: Why Transparency Needs Law, Not Just Tech</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <style>
      /* ==========================================================================
       CSS Variables & Reset
       ========================================================================== */
      :root {
        --color-primary: #1a4d3e;
        --color-primary-dark: #0d3328;
        --color-accent: #2d7a5a;
        --color-accent-light: #3d9970;
        --color-bg: #f5f7f6;
        --color-card: #ffffff;
        --color-text: #1a3330;
        --color-text-muted: #4a6a63;
        --color-border: #d4ddd9;
        --shadow-sm: 0 1px 2px rgba(15, 23, 42, 0.05);
        --shadow-md: 0 4px 6px -1px rgba(15, 23, 42, 0.1),
          0 2px 4px -2px rgba(15, 23, 42, 0.1);
        --shadow-lg: 0 10px 15px -3px rgba(15, 23, 42, 0.1),
          0 4px 6px -4px rgba(15, 23, 42, 0.1);
        --shadow-xl: 0 20px 25px -5px rgba(15, 23, 42, 0.1),
          0 8px 10px -6px rgba(15, 23, 42, 0.1);
        --radius-sm: 0.5rem;
        --radius-md: 0.75rem;
        --radius-lg: 1rem;
        --radius-xl: 1.5rem;
        --max-width: 72rem;
        --nav-height: 4rem;
      }

      *,
      *::before,
      *::after {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      html {
        scroll-behavior: smooth;
        scroll-padding-top: calc(var(--nav-height) + 1.5rem);
      }

      body {
        font-family: "Inter", -apple-system, BlinkMacSystemFont, "Segoe UI",
          Roboto, sans-serif;
        font-size: 1rem;
        line-height: 1.6;
        color: var(--color-text);
        background: var(--color-bg);
      }

      /* ==========================================================================
       Fixed Navigation
       ========================================================================== */
      .nav {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        height: var(--nav-height);
        background: rgba(255, 255, 255, 0.92);
        backdrop-filter: blur(12px);
        border-bottom: 1px solid var(--color-border);
        z-index: 1000;
      }

      .nav-inner {
        max-width: var(--max-width);
        margin: 0 auto;
        padding: 0 1.5rem;
        height: 100%;
        display: flex;
        align-items: center;
        justify-content: space-between;
      }

      .nav-logo {
        font-weight: 700;
        font-size: 1.1rem;
        color: var(--color-primary-dark);
        text-decoration: none;
      }

      .nav-links {
        display: none;
        list-style: none;
        gap: 0.25rem;
      }

      @media (min-width: 768px) {
        .nav-links {
          display: flex;
        }
      }

      .nav-links a {
        display: block;
        padding: 0.5rem 0.85rem;
        font-size: 0.85rem;
        font-weight: 500;
        color: var(--color-text-muted);
        text-decoration: none;
        border-radius: var(--radius-sm);
        transition: background 0.2s, color 0.2s;
      }

      .nav-links a:hover {
        background: rgba(26, 77, 62, 0.08);
        color: var(--color-primary);
      }

      /* ==========================================================================
       Layout Utilities
       ========================================================================== */
      .container {
        max-width: var(--max-width);
        margin: 0 auto;
        padding: 0 1.5rem;
      }

      .section {
        padding: 4rem 0;
      }

      .section-alt {
        background: var(--color-card);
      }

      .section-title {
        font-size: 1.75rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        color: var(--color-text);
      }

      .section-subtitle {
        font-size: 1rem;
        color: var(--color-text-muted);
        margin-bottom: 2rem;
        max-width: 40rem;
      }

      .subsection {
        margin-top: 2.5rem;
      }

      .subsection-title {
        font-size: 1.25rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--color-text);
      }

      /* ==========================================================================
       Hero Section
       ========================================================================== */
      .hero {
        padding-top: calc(var(--nav-height) + 3rem);
        padding-bottom: 4rem;
        background: linear-gradient(
          135deg,
          #0d2818 0%,
          #1a4d3e 30%,
          #2d7a5a 60%,
          #3d9970 100%
        );
        color: #f8fafc;
        position: relative;
        overflow: hidden;
      }

      .hero::before {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: radial-gradient(
            circle at 20% 80%,
            rgba(61, 153, 112, 0.3) 0%,
            transparent 50%
          ),
          radial-gradient(
            circle at 80% 20%,
            rgba(45, 122, 90, 0.25) 0%,
            transparent 40%
          );
        pointer-events: none;
      }

      .hero-inner {
        position: relative;
        z-index: 1;
      }

      .hero-kicker {
        display: inline-block;
        font-size: 0.75rem;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.15em;
        color: #a8e6cf;
        margin-bottom: 1rem;
      }

      .hero-title {
        font-size: clamp(2rem, 5vw, 3rem);
        font-weight: 700;
        line-height: 1.15;
        margin-bottom: 1rem;
        max-width: 48rem;
      }

      .hero-subtitle {
        font-size: 1.125rem;
        color: #c8e6d5;
        max-width: 36rem;
        margin-bottom: 2rem;
        line-height: 1.7;
      }

      .hero-author {
        font-size: 0.9rem;
        color: #a8e6cf;
        margin-bottom: 2.5rem;
      }

      /* Research Question Card */
      .rq-card {
        background: rgba(255, 255, 255, 0.08);
        border: 1px solid rgba(255, 255, 255, 0.15);
        border-radius: var(--radius-lg);
        padding: 1.5rem 1.75rem;
        margin-bottom: 1.5rem;
        backdrop-filter: blur(8px);
      }

      .rq-label {
        font-size: 0.7rem;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.1em;
        color: #fbbf24;
        margin-bottom: 0.5rem;
      }

      .rq-text {
        font-size: 1.1rem;
        font-weight: 500;
        font-style: italic;
        color: #f1f5f9;
        line-height: 1.6;
      }

      /* Executive Summary Card */
      .summary-card {
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: var(--radius-lg);
        padding: 1.5rem 1.75rem;
        margin-bottom: 2rem;
      }

      .summary-label {
        font-size: 0.7rem;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.1em;
        color: #94a3b8;
        margin-bottom: 0.75rem;
      }

      .summary-text {
        color: #e2e8f0;
        line-height: 1.7;
      }

      /* CTA Buttons */
      .hero-ctas {
        display: flex;
        flex-wrap: wrap;
        gap: 0.75rem;
        margin-top: 1.5rem;
      }

      .btn {
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
        padding: 0.75rem 1.5rem;
        font-size: 0.9rem;
        font-weight: 600;
        text-decoration: none;
        border-radius: 999px;
        transition: all 0.2s;
        cursor: pointer;
        border: none;
      }

      .btn-primary {
        background: var(--color-accent);
        color: #fff;
        box-shadow: 0 4px 14px rgba(45, 122, 90, 0.4);
      }

      .btn-primary:hover {
        background: var(--color-accent-light);
        transform: translateY(-1px);
      }

      .btn-secondary {
        background: transparent;
        color: #e2e8f0;
        border: 1px solid rgba(255, 255, 255, 0.3);
      }

      .btn-secondary:hover {
        background: rgba(255, 255, 255, 0.1);
        border-color: rgba(255, 255, 255, 0.5);
      }

      /* ==========================================================================
       Cards
       ========================================================================== */
      .card {
        background: var(--color-card);
        border-radius: var(--radius-lg);
        padding: 1.5rem;
        box-shadow: var(--shadow-md);
        border: 1px solid var(--color-border);
      }

      .card-title {
        font-size: 1.1rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
        color: var(--color-text);
      }

      .card-text {
        font-size: 0.95rem;
        color: var(--color-text-muted);
        line-height: 1.6;
      }

      /* Card Grid */
      .card-grid {
        display: grid;
        gap: 1.25rem;
        grid-template-columns: 1fr;
      }

      @media (min-width: 640px) {
        .card-grid {
          grid-template-columns: repeat(2, 1fr);
        }
      }

      @media (min-width: 1024px) {
        .card-grid {
          grid-template-columns: repeat(3, 1fr);
        }
      }

      .card-grid-4 {
        grid-template-columns: 1fr;
      }

      .card-grid-5 {
        grid-template-columns: 1fr;
      }

      @media (min-width: 640px) {
        .card-grid-4 {
          grid-template-columns: repeat(2, 1fr);
        }
        .card-grid-5 {
          grid-template-columns: repeat(2, 1fr);
        }
      }

      @media (min-width: 1024px) {
        .card-grid-4 {
          grid-template-columns: repeat(4, 1fr);
        }
        .card-grid-5 {
          grid-template-columns: repeat(5, 1fr);
        }
      }

      /* Stakeholder Cards */
      .stakeholder-card {
        background: var(--color-card);
        border-radius: var(--radius-md);
        padding: 1.25rem;
        box-shadow: var(--shadow-sm);
        border: 1px solid var(--color-border);
        transition: box-shadow 0.2s, transform 0.2s;
      }

      .stakeholder-card:hover {
        box-shadow: var(--shadow-lg);
        transform: translateY(-2px);
      }

      .stakeholder-icon {
        width: 2.5rem;
        height: 2.5rem;
        background: linear-gradient(135deg, #e8f5e9, #c8e6c9);
        border-radius: var(--radius-sm);
        display: flex;
        align-items: center;
        justify-content: center;
        margin-bottom: 0.75rem;
        font-size: 1.25rem;
      }

      .stakeholder-title {
        font-size: 0.95rem;
        font-weight: 600;
        margin-bottom: 0.35rem;
        color: var(--color-text);
      }

      .stakeholder-desc {
        font-size: 0.8rem;
        color: var(--color-text-muted);
        line-height: 1.5;
      }

      /* ==========================================================================
       Data & Analysis Section
       ========================================================================== */
      .chart-section {
        margin-bottom: 3rem;
      }

      .chart-section:last-child {
        margin-bottom: 0;
      }

      .chart-card {
        background: #ffffff;
        border-radius: var(--radius-lg);
        padding: 1.5rem;
        box-shadow: var(--shadow-xl);
        border: 1px solid var(--color-border);
      }

      .chart-header {
        margin-bottom: 1rem;
      }

      .chart-title {
        font-size: 1.1rem;
        font-weight: 600;
        color: var(--color-primary-dark);
        margin-bottom: 0.5rem;
      }

      .chart-caption {
        font-size: 0.85rem;
        color: var(--color-text-muted);
        line-height: 1.6;
      }

      .chart-container {
        min-height: 400px;
        background: rgba(30, 41, 59, 0.5);
        border-radius: var(--radius-md);
        display: flex;
      }

      .chart-takeaway {
        margin-top: 1.5rem;
        padding: 1.25rem;
        background: rgba(26, 77, 62, 0.08);
        border-left: 3px solid var(--color-primary);
        border-radius: var(--radius-sm);
      }

      .chart-takeaway h4 {
        font-size: 0.9rem;
        font-weight: 600;
        color: var(--color-primary);
        margin-bottom: 0.75rem;
      }

      .chart-takeaway p {
        font-size: 0.9rem;
        color: var(--color-text);
        line-height: 1.7;
        align-items: center;
        justify-content: center;
        color: #64748b;
        font-size: 0.9rem;
        border: 1px dashed #334155;
      }

      /* ==========================================================================
       Policy Options Section
       ========================================================================== */
      .policy-options {
        display: grid;
        gap: 1.5rem;
        grid-template-columns: 1fr;
      }

      @media (min-width: 768px) {
        .policy-options {
          grid-template-columns: repeat(3, 1fr);
        }
      }

      .policy-card {
        background: var(--color-card);
        border-radius: var(--radius-lg);
        padding: 1.5rem;
        box-shadow: var(--shadow-md);
        border: 1px solid var(--color-border);
        position: relative;
      }

      .policy-desc {
        font-size: 0.9rem;
        color: var(--color-text-muted);
        margin-bottom: 1.25rem;
        line-height: 1.6;
      }

      .policy-card.recommended {
        border-color: var(--color-primary);
        box-shadow: 0 0 0 3px rgba(26, 77, 62, 0.15), var(--shadow-lg);
      }

      .policy-card.recommended::before {
        content: "‚òÖ Recommended";
        position: absolute;
        top: -0.75rem;
        left: 1rem;
        background: var(--color-primary);
        color: white;
        font-size: 0.7rem;
        font-weight: 600;
        padding: 0.25rem 0.75rem;
        border-radius: 999px;
        text-transform: uppercase;
        letter-spacing: 0.05em;
      }

      .policy-number {
        font-size: 0.7rem;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.1em;
        color: var(--color-text-muted);
        margin-bottom: 0.5rem;
      }

      .policy-title {
        font-size: 1.1rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--color-text);
      }

      .policy-pros,
      .policy-risks {
        margin-bottom: 1rem;
      }

      .policy-label {
        font-size: 0.75rem;
        font-weight: 600;
        text-transform: uppercase;
        letter-spacing: 0.05em;
        margin-bottom: 0.5rem;
      }

      .policy-label.pros {
        color: #16a34a;
      }

      .policy-label.risks {
        color: #dc2626;
      }

      .policy-list {
        list-style: none;
        padding: 0;
      }

      .policy-list li {
        font-size: 0.85rem;
        color: var(--color-text-muted);
        padding-left: 1rem;
        position: relative;
        margin-bottom: 0.35rem;
      }

      .policy-list li::before {
        content: "‚Ä¢";
        position: absolute;
        left: 0;
        color: var(--color-text-muted);
      }

      /* Recommendation Box */
      .recommendation-box {
        background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
        border-left: 4px solid var(--color-primary);
        border-radius: var(--radius-md);
        padding: 1.5rem 1.75rem;
        margin-top: 2.5rem;
      }

      .recommendation-label {
        font-size: 0.75rem;
        font-weight: 700;
        text-transform: uppercase;
        letter-spacing: 0.1em;
        color: var(--color-primary);
        margin-bottom: 0.75rem;
      }

      .recommendation-text {
        font-size: 1rem;
        color: var(--color-text);
        line-height: 1.7;
      }

      /* ==========================================================================
       Methods Section
       ========================================================================== */
      .methods-grid {
        display: grid;
        gap: 2rem;
        grid-template-columns: 1fr;
      }

      @media (min-width: 768px) {
        .methods-grid {
          grid-template-columns: 1fr 1fr;
        }
      }

      .methods-column h3 {
        font-size: 1rem;
        font-weight: 600;
        margin-bottom: 1rem;
        color: var(--color-text);
      }

      .methods-list {
        list-style: none;
        padding: 0;
      }

      .methods-list li {
        font-size: 0.9rem;
        color: var(--color-text-muted);
        padding: 0.5rem 0;
        border-bottom: 1px solid var(--color-border);
      }

      .methods-list li:last-child {
        border-bottom: none;
      }

      /* ==========================================================================
       References & About
       ========================================================================== */
      .references-list {
        list-style: decimal;
        padding-left: 1.5rem;
      }

      .references-list li {
        font-size: 0.9rem;
        color: var(--color-text-muted);
        padding: 0.5rem 0;
        line-height: 1.6;
      }

      .about-box {
        background: var(--color-card);
        border-radius: var(--radius-lg);
        padding: 1.5rem 1.75rem;
        margin-top: 2.5rem;
        box-shadow: var(--shadow-sm);
        border: 1px solid var(--color-border);
      }

      .about-title {
        font-size: 1rem;
        font-weight: 600;
        margin-bottom: 0.75rem;
        color: var(--color-text);
      }

      .about-text {
        font-size: 0.9rem;
        color: var(--color-text-muted);
        line-height: 1.7;
      }

      /* ==========================================================================
       Footer
       ========================================================================== */
      .footer {
        background: #0d2818;
        color: #a8c5b5;
        padding: 2rem 0;
        text-align: center;
        font-size: 0.85rem;
      }

      .footer a {
        color: #c8e6d5;
        text-decoration: none;
      }

      .footer a:hover {
        text-decoration: underline;
      }

      /* ==========================================================================
       Utility Classes
       ========================================================================== */
      .text-muted {
        color: var(--color-text-muted);
      }

      .mt-1 {
        margin-top: 0.5rem;
      }
      .mt-2 {
        margin-top: 1rem;
      }
      .mt-3 {
        margin-top: 1.5rem;
      }
      .mt-4 {
        margin-top: 2rem;
      }
      .mb-1 {
        margin-bottom: 0.5rem;
      }
      .mb-2 {
        margin-bottom: 1rem;
      }
      .mb-3 {
        margin-bottom: 1.5rem;
      }
      .mb-4 {
        margin-bottom: 2rem;
      }

      /* Prose styling for longer text blocks */
      .prose p {
        margin-bottom: 1rem;
        line-height: 1.7;
      }

      .prose p:last-child {
        margin-bottom: 0;
      }
    </style>
  </head>
  <body>
    <!-- ========================================================================
       Fixed Navigation
       ======================================================================== -->
    <nav class="nav">
      <div class="nav-inner">
        <a href="#" class="nav-logo">AI Trust & Regulation</a>
        <ul class="nav-links">
          <li><a href="#hero">Overview</a></li>
          <li><a href="#background">Background</a></li>
          <li><a href="#data">Data & Analysis</a></li>
          <li><a href="#policy">Policy Options</a></li>
          <li><a href="#methods">Methods</a></li>
          <li><a href="#references">References</a></li>
        </ul>
      </div>
    </nav>

    <!-- ========================================================================
       Section 0: Hero & Research Question
       ======================================================================== -->
    <section id="hero" class="hero">
      <div class="container hero-inner">
        <span class="hero-kicker">AI, Public Attention, and the Trust Gap</span>

        <h1 class="hero-title">
          The AI Trust Gap: Why Voluntary Transparency Isn't Enough
        </h1>

        <p class="hero-subtitle">
          Data practices in LLMs are reshaping power, privacy, and public trust.
          This project examines whether corporate transparency efforts and
          rising public attention meaningfully contribute to public trust in
          AI‚Äîand what gaps emerge when trust does not follow.
        </p>

        <p class="hero-author">
          By Sixing Tao ¬∑ University of Washington ¬∑ 2025
        </p>

        <!-- Research Question Card -->
        <div class="rq-card">
          <div class="rq-label">Research Question</div>
          <p class="rq-text">
            How does rising public attention to AI relate to levels of public
            trust, and what role might government regulation play relative to
            voluntary transparency in shaping this relationship?
          </p>
        </div>

        <!-- Executive Summary Card -->
        <div class="summary-card">
          <div class="summary-label">Executive Summary</div>
          <p class="summary-text">
            As AI technologies rapidly advance, public attention to AI has
            increased dramatically. However, this surge in attention has not
            been accompanied by a proportional rise in concern for AI safety,
            privacy, or long-term risk. Although this year, we have seen a
            relative increase in safety-related discourse, the absolute volume
            of such concern remains small compared to the scale of technological
            expansion.
          </p>
          <p>
            In this context, existing responses from both corporations and
            governments remain insufficient. Voluntary transparency efforts and
            high-level policy principles have not translated into increased
            public trust. Despite greater visibility into AI systems, the public
            has not meaningfully begun to trust AI technologies as safe,
            accountable, or aligned with societal values. This gap is not
            surprising. Historically, transformative technologies‚Äîfrom
            pharmaceuticals to aviation to digital finance‚Äîdid not gain
            widespread public trust through corporate self-regulation alone.
            Instead, trust emerged when governments institutionalized safeguards
            through enforceable regulation. Regulation has repeatedly functioned
            not as a constraint on innovation, but as a condition that enables
            public acceptance and sustainable technological development.
          </p>
          <p>
            From this perspective, accelerating government-led AI regulation is
            not premature‚Äîit is overdue. Governments should move beyond
            high-level principles and voluntary frameworks and rapidly develop
            concrete, enforceable regulations that can translate public concern
            into durable trust in AI systems.
          </p>
        </div>

        <!-- CTA Buttons -->
        <div class="hero-ctas">
          <a href="#data" class="btn btn-primary">View Data & Analysis</a>
          <a href="#policy" class="btn btn-secondary">See Policy Options</a>
        </div>
      </div>
    </section>

    <!-- ========================================================================
       Section 1: Background & Problem Statement
       ======================================================================== -->
    <section id="background" class="section section-alt">
      <div class="container">
        <h2 class="section-title">Background & Problem Statement</h2>
        <p class="section-subtitle">
          Situating data practices, stakeholders, and governance in the emerging
          AI trust gap.
        </p>

        <!-- 2.1 The Rise of Generative AI -->
        <div class="subsection">
          <h3 class="subsection-title">
            2.1 Data Practices Behind Generative AI
          </h3>
          <div class="prose">
            <p>
              Generative AI systems such as GPT-4, Claude, and Gemini have
              rapidly become embedded in search engines, productivity tools, and
              creative platforms. While these systems are often presented
              through the lens of model performance and user-facing
              capabilities, their operation is fundamentally shaped by
              underlying data practices. Decisions about what data is collected,
              how it is processed over time, and who retains control over its
              use play a central role in determining not only how these systems
              function, but also how they are perceived by the public. As
              generative AI becomes increasingly infrastructural, questions
              surrounding data governance have emerged as a key dimension of
              public trust.
            </p>

            <p>
              At the training stage, generative AI models are typically built on
              web-scale datasets that combine public websites, books, academic
              literature, social media content, and other forms of
              user-generated data. Much of this material is incorporated without
              explicit consent from original creators, relying instead on broad
              interpretations of public availability or permissibility. These
              practices blur the boundary between public and private information
              and complicate traditional notions of ownership and control. Once
              absorbed into large-scale models, individual contributions become
              difficult to trace, audit, or remove, limiting meaningful recourse
              for those whose data shapes system behavior.
            </p>

            <p>
              During deployment and use, additional data is generated through
              user interactions, including prompts, responses, and behavioral
              metadata. Although many AI companies assert that such data is
              handled securely or used to improve system quality and safety, the
              specifics of retention, reuse, and downstream access remain opaque
              to most users. In response to growing scrutiny, firms have
              increased voluntary transparency efforts by publishing model
              cards, documentation, and policy statements. However, these
              disclosures are largely self-directed, uneven across companies,
              and lack standardized oversight or independent verification. As a
              result, increased visibility into data practices has not
              necessarily translated into greater public confidence,
              highlighting a structural gap between transparency and trust.
            </p>
          </div>
        </div>

        <!-- 2.2 Stakeholders -->
        <div class="subsection">
          <h3 class="subsection-title">2.2 Key Stakeholders</h3>
          <div class="card-grid card-grid-4">
            <div class="stakeholder-card">
              <div class="stakeholder-icon">üè¢</div>
              <h4 class="stakeholder-title">AI Companies</h4>
              <p class="stakeholder-desc">
                Design, train, and deploy large language models using web-scale
                datasets. While firms increasingly emphasize transparency
                through documentation and policy statements, they retain
                substantial discretion over what data practices are disclosed
                and how risks are framed. Competitive pressures and liability
                concerns create incentives for selective transparency, limiting
                the extent to which voluntary disclosure can foster public
                trust.
              </p>
            </div>

            <div class="stakeholder-card">
              <div class="stakeholder-icon">‚öñÔ∏è</div>
              <h4 class="stakeholder-title">Governments & Regulators</h4>
              <p class="stakeholder-desc">
                Tasked with safeguarding public interests by establishing
                enforceable standards for data use, accountability, and
                oversight. Regulators face the challenge of responding to
                rapidly evolving AI systems while relying on information largely
                produced by industry actors themselves. Delays or gaps in
                regulatory action can leave trust-building mechanisms dependent
                on voluntary corporate practices.
              </p>
            </div>

            <div class="stakeholder-card">
              <div class="stakeholder-icon">üë•</div>
              <h4 class="stakeholder-title">The Public</h4>
              <p class="stakeholder-desc">
                Data subjects whose information may be incorporated into model
                training and end users who interact with AI systems in everyday
                contexts. Although directly affected by data practices, members
                of the public typically lack visibility into how their data is
                collected, retained, or reused, and have limited ability to
                meaningfully consent, opt out, or seek redress.
              </p>
            </div>

            <div class="stakeholder-card">
              <div class="stakeholder-icon">üîç</div>
              <h4 class="stakeholder-title">Civil Society & Researchers</h4>
              <p class="stakeholder-desc">
                Journalists, academic researchers, and advocacy organizations
                who seek to evaluate AI systems and hold developers accountable.
                Their ability to assess data practices and risks is constrained
                by restricted access to training data, system logs, and internal
                documentation, limiting the effectiveness of external scrutiny
                in the absence of formal regulatory authority.
              </p>
            </div>
          </div>
        </div>

        <!-- 2.3 Why This Matters Now -->
        <div class="subsection">
          <h3 class="subsection-title">2.3 Why This Matters Now</h3>
          <div class="prose">
            <p>
              What distinguishes the current moment is not simply the explosive
              and widespread use of generative AI, but the speed at which data
              practices are becoming normalized before durable governance
              mechanisms are in place. Decisions about data collection,
              retention, and reuse are increasingly embedded into production
              systems, often solidifying as default practices rather than
              deliberate policy choices.
            </p>
            <p>
              This sequencing creates a structural risk. When transparency
              frameworks and regulatory oversight lag behind deployment, early
              design decisions can lock in asymmetric power relationships
              between developers, users, and external watchdogs. Once public
              trust erodes under these conditions, it is difficult to restore
              through retrospective disclosure alone. The present moment
              therefore represents a narrow window in which governance
              interventions can still meaningfully shape how trust in AI systems
              is formed.
            </p>
          </div>

          <!-- Problem Statement Callout -->
          <div class="recommendation-box" style="margin-top: 2rem">
            <div class="recommendation-label">üìã Problem Statement</div>
            <p class="recommendation-text">
              Public attention to AI has risen sharply alongside expanded
              corporate transparency efforts, yet this increase has not been
              accompanied by corresponding gains in public trust or perceived
              control over AI systems. This divergence raises a central
              governance problem: greater visibility into AI practices does not
              necessarily translate into greater confidence in their safety,
              accountability, or alignment with societal values. The persistence
              of this trust gap suggests that reliance on voluntary corporate
              disclosure alone may be structurally insufficient, prompting the
              need to examine whether and how government regulation might play a
              distinct role in shaping public trust in AI.
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- ========================================================================
       Section 2: Data & Analysis (Dashboard)
       ======================================================================== -->
    <section id="data" class="section">
      <div class="container">
        <h2 class="section-title">Data & Analysis</h2>
        <p class="section-subtitle">
          Three visualizations document the AI trust gap: the pace of
          development, the lag in policy attention, and the failure of
          transparency to build trust.
        </p>

        <!-- 3.1 AI Evolution Timeline -->
        <div class="chart-section">
          <div class="chart-card">
            <div class="chart-header">
              <h3 class="chart-title">3.1 AI Evolution Timeline</h3>
              <p class="chart-caption">
                This interactive timeline tracks public attention to AI from
                2020‚Äì2025, combining search interest trends with key AI
                milestones. The trend lines on the left show relative search
                interest for technology terms (ChatGPT, Claude, Gemini, AI
                models), while the right side tracks governance-related searches
                (AI safety, privacy, ethics, bias, trust). Values are normalized
                within each search term, so the left and right scales are not
                directly comparable in magnitude. Events along the central
                timeline include major model launches, breakthrough research
                papers, corporate announcements, cultural moments, and policy
                interventions, color-coded by category. Hover over dots for
                details; use the category filters to focus on specific event
                types; drag to zoom into specific time periods.
              </p>
            </div>

            <!-- Interactive Timeline Visualization -->
            {% include interactivegraph1.html %}

            <!-- Takeaway -->
            <div class="chart-takeaway">
              <h4>üìä Key Takeaway</h4>
              <p>
                Across the timeline, public attention to AI technologies and to
                governance-related topics exhibits distinct temporal shapes.
                Attention to technology-oriented terms expands into a broad,
                outward-growing profile, with a major inflection beginning in
                late 2022 following the release of ChatGPT and the rapid public
                uptake of generative AI systems, after which attention remains
                elevated and sustained over time. In contrast, attention to
                governance-related terms rises more gradually overall and is
                punctuated by a small number of pronounced spikes, with the most
                substantial surge occurring from late May through early October
                2025. Despite these episodic bursts of intensity, the overall
                trajectory reflects a steady and gradual upward trend, with
                sustained growth rather than abrupt or continuously accelerating
                expansion.
              </p>
            </div>
          </div>
        </div>

        <!-- 3.2 Tech vs Governance Attention Trends -->
        <div class="chart-section">
          <div class="chart-card">
            <div class="chart-header">
              <h3 class="chart-title">
                3.2 Tech vs Governance Attention Trends
              </h3>
              <p class="chart-caption">
                Unlike the timeline view, which highlights relative trends and
                temporal patterns, this comparison makes differences in
                magnitude explicit. While public attention to AI as a general
                technology rises sharply and remains elevated, attention to AI
                safety remains consistently negligible by comparison, never
                approaching the scale of overall AI-related interest. Across the
                entire period, search activity related to AI safety accounts for
                only a tiny fraction of total AI attention, remaining orders of
                magnitude lower than interest in AI itself.
              </p>
            </div>
            <script
              type="text/javascript"
              src="https://ssl.gstatic.com/trends_nrtr/4284_RC01/embed_loader.js"
            ></script>
            <script type="text/javascript">
              trends.embed.renderExploreWidget(
                "TIMESERIES",
                {
                  comparisonItem: [
                    { keyword: "AI", geo: "US", time: "today 5-y" },
                    { keyword: "AI safety", geo: "US", time: "today 5-y" },
                  ],
                  category: 0,
                  property: "",
                },
                {
                  exploreQuery:
                    "date=today%205-y&geo=US&q=AI,AI%20safety&hl=en",
                  guestPath: "https://trends.google.com:443/trends/embed/",
                }
              );
            </script>
            <p class="chart-caption">
              This dual-chart visualization analyzes the temporal relationship
              between technology and governance attention using the same
              underlying data: weekly Google Trends search volumes for
              technology terms (ChatGPT, Claude, Gemini, AI, DeepSeek) and
              governance terms (AI safety, AI privacy, AI ethics, AI bias, AI
              trust). The upper chart displays standardized trend indices over
              time, where each keyword is first z-scored within its own time
              series to remove scaling differences, then averaged to create
              composite TechIndex (blue) and GovIndex (orange) measures. This
              normalization reveals patterns of co-movement rather than absolute
              magnitude. Hover over the lines to see exact index values and
              dates; toggle the 4-week rolling mean to smooth short-term
              fluctuations. The lower chart computes lagged Pearson correlations
              across different time offsets: positive lags indicate governance
              attention lagging behind technology attention, while negative lags
              indicate the reverse. The peak correlation (marked with a red
              line) identifies the optimal time lag that maximizes alignment
              between the two series. Use the max lag slider to adjust the
              analysis window; drag to select a lag range and view the average
              correlation within that interval.
            </p>

            <!-- Lagged Cross-Correlation Chart -->
            {% include LagCorrelationChart.html %}

            <!-- Takeaway -->
            <div class="chart-takeaway">
              <h4>üìä Key Takeaway</h4>
              <p>
                Together, these visualizations show a persistent imbalance
                between technological and governance attention around AI. While
                public interest in AI technologies has expanded rapidly and
                remained high, governance-related attention remains orders of
                magnitude smaller and largely episodic. Although standardized
                trend indices reveal a high overall correlation between
                technology and governance attention, lagged cross-correlation
                analysis does not identify a stable or consistent delay by which
                governance attention follows technological developments.
                Correlation remains relatively flat indicating that governance
                discourse does not respond to technological change in a regular
                or predictable temporal pattern. Instead, the observed
                correlation is primarily driven by shared long-term trends
                rather than a proportional or timely governance response.
              </p>
            </div>
          </div>
        </div>

        <!-- 3.3 The Attention-Trust-Transparency Disconnect -->
        <div class="chart-section">
          <div class="chart-card">
            <div class="chart-header">
              <h3 class="chart-title">
                3.3 The Attention‚ÄìTrust‚ÄìTransparency Disconnect
              </h3>
              <p class="chart-caption">
                This visualization tracks four data dimensions over time to
                examine a critical disconnect between attention, transparency,
                and trust: (1) Public Sentiment ‚Äî three lines showing the
                percentage of Americans who are ‚Äúmore concerned‚Äù (red), ‚Äúequally
                concerned and excited‚Äù (orange), or ‚Äúmore excited‚Äù (blue) about
                AI, based on Pew Research surveys from 2021 to 2025; (2) AI
                Search Interest (gray shaded area) ‚Äî Google Trends search volume
                for ‚ÄúAI‚Äù as a proxy for public attention; and (3) Industry
                Transparency Score (green step line) ‚Äî the average score across
                major AI companies (OpenAI, Google, Anthropic, Meta, Amazon,
                Stability AI, AI2 Lab), based on disclosures related to training
                data, model cards, and safety reports. Transparency data
                consists of two snapshot assessments (October 2023 and May 2024)
                and is therefore visualized as step changes rather than a
                continuous trend, with horizontal segments indicating constant
                scores between assessments and vertical jumps representing
                measured updates. Hover interactions allow inspection of exact
                sentiment values and company-level transparency breakdowns at
                the two snapshot dates. The key question this visualization
                raises is whether rising public attention and increasing
                industry transparency are accompanied by declining concern and
                growing excitement; in this analysis, public trust is
                operationalized using expressed concern as an inverse proxy,
                reflecting heightened perceived risk and diminished trust in
                emerging AI technologies.
              </p>
            </div>

            <!-- Attention-Trust-Transparency Chart -->
            {% include AttentionTrustChart.html %}

            <!-- Takeaway -->
            <div class="chart-takeaway">
              <h4>üìä Key Takeaway</h4>
              <p>
                Across this period, attention to AI technologies and industry
                transparency efforts both rise substantially, yet public
                sentiment does not follow a corresponding trajectory toward
                reduced concern or increased excitement. The share of Americans
                expressing concern remains elevated and stable over time, while
                the share expressing excitement declines and stays low. The
                proportion holding mixed views also decreases, suggesting a
                gradual polarization of sentiment, predominantly toward concern.
                This pattern reveals a disconnect between the expansion of
                voluntary corporate disclosure and actual shifts in public
                trust. Rather than building confidence, increased transparency
                in its current, company-controlled form appears insufficient to
                address the underlying drivers of public unease. The observed
                dynamics suggest that transparency alone, without independent
                verification or institutional accountability, may not translate
                into meaningful trust.
              </p>
            </div>
          </div>
        </div>

        <!-- 3.4 Why Transparency Alone Failed -->
        <div class="chart-section">
          <div class="chart-card">
            <div class="chart-header">
              <h3 class="chart-title">
                3.4 Why Transparency Alone Failed to Build Trust
              </h3>
            </div>
            <div class="prose">
              <p>
                The three visualizations above tell a consistent story: public
                attention to AI has surged, corporate transparency efforts have
                increased, yet trust has not improved. Why not?
              </p>
              <p>
                Many policy discussions assume a simple relationship:
                <em>more information ‚Üí more trust</em>. But these visualizations
                show that isn't happening in practice. The naive assumption that
                disclosure automatically builds confidence fails to account for
                several factors: the complexity and technical nature of AI
                documentation, the inherent conflict of interest when companies
                control their own disclosures, the absence of independent
                verification, and the lack of meaningful consequences for
                incomplete or misleading information.
              </p>
              <p>
                Historical evidence from other technological domains suggests
                that public trust rarely emerges from transparency alone, but
                rather from institutional frameworks that render transparency
                credible, enforceable, and independently verifiable. In
                <strong>food safety</strong>, public confidence did not arise
                because companies voluntarily disclosed ingredients or safety
                practices, but because regulatory bodies such as the U.S. Food
                and Drug Administration (FDA) established mandatory standards,
                inspection regimes, and recall authorities that transformed
                safety claims into enforceable public guarantees (<a
                  href="https://www.fda.gov/about-fda/what-we-do"
                  target="_blank"
                  >FDA, What We Do</a
                >). Trust followed not disclosure itself, but the presence of
                oversight, enforcement, and consequences for failure.
              </p>

              <p>
                A similar pattern appears in <strong>financial markets</strong>.
                Investor trust was not restored through voluntary corporate
                transparency, but through the creation of the U.S. Securities
                and Exchange Commission (SEC) after the Great Depression, which
                mandated standardized financial reporting and required
                independent external audits (<a
                  href="https://www.sec.gov/about.shtml"
                  target="_blank"
                  >SEC, What We Do</a
                >). Decades of research show that such regulatory institutions
                reduce information asymmetry and fraud, enabling markets to
                function precisely because disclosures are validated by
                independent verification rather than goodwill alone (<a
                  href="https://doi.org/10.1111/1475-679X.12115"
                  target="_blank"
                  >Leuz & Wysocki, 2016</a
                >).
              </p>

              <p>
                In <strong>aviation</strong>, widespread public trust in
                commercial flight emerged not from airlines publishing safety
                information, but from continuous certification, incident
                reporting, and independent oversight by agencies such as the
                Federal Aviation Administration (FAA). The FAA's regulatory
                framework‚Äîcovering aircraft certification, pilot licensing, and
                mandatory incident investigation‚Äîhas been repeatedly cited as a
                cornerstone of aviation's safety record (<a
                  href="https://www.faa.gov/about/mission/activities"
                  target="_blank"
                  >FAA, Mission and Activities</a
                >). Here again, trust was not the product of transparency in
                isolation, but of institutions that aligned private incentives
                with public risk tolerance.
              </p>

              <p>
                Across these domains, government intervention did not eliminate
                risk nor guarantee trust. Instead, it created the structural
                conditions under which trust could reasonably form by reducing
                information asymmetry, enforcing consequences, and providing
                mechanisms for independent verification. These historical
                precedents suggest that governance can play a critical role not
                by replacing transparency, but by making it meaningful,
                comparable, and trustworthy‚Äîan insight that is particularly
                salient for contemporary AI systems.
              </p>
              <p>
                <strong>To be clear:</strong> this analysis does NOT prove that
                government intervention automatically builds trust. Regulation
                can be poorly designed, captured by industry, or out of step
                with technological realities. What the data <em>does</em> show
                is that current voluntary approaches have not succeeded. This
                creates a <strong>governance gap</strong> that justifies
                exploring institutional solutions‚Äînot with certainty that they
                will work, but with recognition that the status quo is failing.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- ========================================================================
       Section 3: Policy Options & Recommendation
       ======================================================================== -->
    <section id="policy" class="section section-alt">
      <div class="container">
        <h2 class="section-title">
          Policy Options: Who Should Be Responsible for Building AI Trust?
        </h2>
        <p class="section-subtitle">
          The data shows that voluntary transparency has not built trust. The
          question now is: what institutional arrangements might do better? Here
          we consider three approaches, each with distinct assumptions about
          where responsibility should lie.
        </p>

        <div class="policy-options">
          <!-- Option 1: Market-Led (Status Quo) -->
          <div class="policy-card">
            <div class="policy-number">Option 1</div>
            <h3 class="policy-title">Market-Led Transparency (Status Quo)</h3>
            <p class="policy-desc">
              Governments do not directly intervene. Companies continue to
              voluntarily publish Model Cards, System Cards, and safety reports.
              Market mechanisms‚Äîuser choice, reputational risk, competitive
              pressure‚Äîserve as the primary constraining forces.
            </p>

            <div class="policy-pros">
              <div class="policy-label pros">Rationale</div>
              <ul class="policy-list">
                <li>
                  Maximizes innovation speed by avoiding regulatory constraints
                </li>
                <li>
                  Addresses the "pacing problem"‚Äîregulation may lag behind
                  fast-moving technology
                </li>
                <li>
                  Appropriate for early-stage technologies with rapid iteration
                  cycles
                </li>
              </ul>
            </div>

            <div class="policy-risks">
              <div class="policy-label risks">Critique (Based on Analysis)</div>
              <ul class="policy-list">
                <li>
                  Sections 3.3 and 3.4 demonstrate that transparency has
                  increased while trust has not
                </li>
                <li>
                  This option is logically invalidated by the evidence:
                  voluntary disclosure has failed to close the trust gap
                </li>
                <li>
                  Leads to persistent information asymmetry and potential
                  "tragedy of the commons" dynamics
                </li>
                <li>
                  No independent verification or meaningful consequences for
                  incomplete disclosure
                </li>
              </ul>
            </div>
          </div>

          <!-- Option 2: Co-Regulation & Audited Standards (Recommended) -->
          <div class="policy-card recommended">
            <div class="policy-number">Option 2 ‚Äî Recommended</div>
            <h3 class="policy-title">Co-Regulation & Audited Standards</h3>
            <p class="policy-desc">
              Governments set high-level principles and mandatory objectives
              (e.g., "models must pass independent safety testing"), while
              specific implementation standards and technical details are
              jointly developed by industry and academia (similar to NIST
              frameworks). Third-party auditing serves as the verification
              mechanism.
            </p>

            <div class="policy-pros">
              <div class="policy-label pros">Rationale</div>
              <ul class="policy-list">
                <li>
                  Balances the tension between "governments lack technical
                  expertise" and "companies lack public credibility"
                </li>
                <li>
                  Solves the verification problem through independent
                  third-party audits
                </li>
                <li>
                  Avoids regulatory rigidity‚Äîtechnical standards can be updated
                  flexibly as technology evolves, unlike legislation
                </li>
                <li>
                  Analogous to the financial auditing model: government mandates
                  audits, but standards are set by professional bodies
                </li>
              </ul>
            </div>

            <div class="policy-risks">
              <div class="policy-label risks">Caveats & Risks</div>
              <ul class="policy-list">
                <li>
                  Requires robust institutions to prevent industry capture of
                  standard-setting processes
                </li>
                <li>
                  Third-party auditor competence and independence must be
                  ensured
                </li>
                <li>
                  May still be too slow for the fastest-moving frontier systems
                </li>
              </ul>
            </div>
          </div>

          <!-- Option 3: Centralized Oversight & Pre-Market Licensing -->
          <div class="policy-card">
            <div class="policy-number">Option 3</div>
            <h3 class="policy-title">
              Centralized Oversight & Pre-Market Licensing
            </h3>
            <p class="policy-desc">
              Establish a dedicated AI regulatory agency (analogous to the FDA
              or FAA) that implements a pre-market approval regime. Frontier
              models must undergo government testing and approval before public
              release; unlicensed systems cannot be deployed.
            </p>

            <div class="policy-pros">
              <div class="policy-label pros">Rationale</div>
              <ul class="policy-list">
                <li>
                  Most powerful form of oversight, directly corresponding to the
                  FDA/FAA precedents cited in Section 3.4
                </li>
                <li>
                  Maximizes prevention of unsafe products reaching the public
                </li>
                <li>
                  Grants government direct enforcement authority with clear
                  consequences
                </li>
              </ul>
            </div>

            <div class="policy-risks">
              <div class="policy-label risks">Caveats & Risks</div>
              <ul class="policy-list">
                <li>
                  May create rent-seeking opportunities and dramatically
                  increase compliance costs
                </li>
                <li>
                  Could favor large incumbents and stifle open-source
                  communities and startups
                </li>
                <li>
                  Governments may lack sufficient compute capacity and technical
                  talent to evaluate frontier models
                </li>
                <li>
                  Risk of regulatory lag‚Äîapproval processes may not keep pace
                  with rapid model iteration
                </li>
              </ul>
            </div>
          </div>
        </div>

        <!-- Recommendation Box -->
        <div class="recommendation-box">
          <div class="recommendation-label">üìå Recommendation</div>
          <p class="recommendation-text">
            Based on the observed disconnect between attention, transparency,
            and trust, we recommend moving beyond purely market-led transparency
            toward a
            <strong>co-regulation and audited standards framework</strong>.
            Option 1 (status quo) is logically invalidated by the evidence
            presented in Sections 3.3 and 3.4: voluntary disclosure has
            increased, yet trust has not followed. Option 3 (centralized
            pre-market licensing) offers the strongest oversight but risks
            regulatory capture, prohibitive compliance costs, and exclusion of
            open-source and startup innovation. Option 2 strikes a practical
            balance: governments set enforceable objectives while industry and
            academia collaboratively develop technical standards, with
            independent third-party audits providing the verification mechanism
            that voluntary disclosure lacks. This approach mirrors the financial
            auditing model‚Äîmandated oversight with flexible, expert-driven
            implementation. The key insight is not that regulation guarantees
            trust, but that
            <strong
              >current voluntary approaches have demonstrably failed</strong
            >. When the status quo is not working, it is reasonable to explore
            institutional alternatives that combine accountability with
            adaptability.
          </p>
        </div>
      </div>
    </section>

    <!-- ========================================================================
       Section 4: Data Sources & Limitations
       ======================================================================== -->
    <section id="methods" class="section">
      <div class="container">
        <h2 class="section-title">Data Sources & Limitations</h2>
        <p class="section-subtitle">
          How we collected, processed, and analyzed data for this policy brief.
        </p>

        <div class="methods-grid">
          <div class="methods-column">
            <h3>Data Sources</h3>
            <ul class="methods-list">
              <li>
                <strong
                  ><a href="https://trends.google.com/trends/" target="_blank"
                    >Google Trends</a
                  >:</strong
                >
                Search interest data for AI technology terms ("ChatGPT," "GPT,"
                "AI model," "Claude," "Gemini") and governance terms ("AI
                regulation," "AI safety," "AI ethics," "AI privacy") from
                2020‚Äì2025.
              </li>
              <li>
                <strong
                  ><a
                    href="https://www.pewresearch.org/wp-content/uploads/sites/20/2025/09/PS_2025.9.17_AI-and-its-impact_topline.pdf"
                    target="_blank"
                    >Pew Research Center</a
                  >:</strong
                >
                Survey data on American attitudes toward AI, including comfort
                levels with AI in various applications and general sentiment
                toward AI development (2021‚Äì2025 surveys).
              </li>
              <li>
                <strong
                  ><a href="https://ai-timeline.org" target="_blank"
                    >AI Timeline Events</a
                  >:</strong
                >
                Compiled from news sources, company announcements, academic
                publications, and policy documents. Events categorized as model
                releases, research breakthroughs, business developments,
                cultural moments, and policy interventions.
              </li>
              <li>
                <strong
                  ><a
                    href="https://crfm.stanford.edu/fmti/May-2024/index.html"
                    target="_blank"
                    >Foundation Model Transparency Index</a
                  >:</strong
                >
                Stanford CRFM's assessment of transparency across major AI
                companies (OpenAI, Google, Anthropic, Meta, Amazon, Stability
                AI, AI2 Lab), based on disclosures related to training data,
                model cards, and safety reports.
              </li>
            </ul>
          </div>

          <div class="methods-column">
            <h3>Limitations</h3>
            <ul class="methods-list">
              <li>
                <strong>Normalization:</strong> All trend data normalized to
                0‚Äì100 scale for cross-source comparison. Only relative patterns
                and timing are meaningful, not absolute values.
              </li>
              <li>
                <strong>Timeline Construction:</strong> Events were manually
                curated based on significance in industry reporting and academic
                literature. The selection is not exhaustive‚Äîsome relevant events
                may be missing, and categorization is subjective.
              </li>
              <li>
                <strong>Data Proxies:</strong> Google Trends measures search
                interest, not actual knowledge or engagement. Survey data is
                self-reported and subject to framing effects. The transparency
                index is a rough proxy‚Äîquality of disclosures varies
                significantly across companies.
              </li>
              <li>
                <strong>Trust Operationalization:</strong> This analysis uses
                expressed concern as an inverse proxy for trust. While concern
                reflects perceived risk and unease, it may not fully capture the
                multidimensional nature of trust. Alternative measures could
                yield different conclusions.
              </li>
              <li>
                <strong>Causal Claims:</strong> This analysis shows correlations
                and patterns, NOT causal relationships. We cannot prove that
                voluntary transparency <em>caused</em> trust stagnation, only
                that the two coexist.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- ========================================================================
       Section 5: About This Project
       ======================================================================== -->
    <section id="about" class="section section-alt">
      <div class="container">
        <!-- About This Project -->
        <div class="about-box">
          <h3 class="about-title">About This Project</h3>
          <p class="about-text">
            This policy brief examines the relationship between public
            attention, corporate transparency, and trust in AI systems. Using
            Google Trends data, Pew Research surveys, and industry transparency
            assessments, the analysis documents a persistent disconnect: while
            public attention to AI has surged and corporate transparency efforts
            have expanded, public concern remains elevated and excitement has
            declined. Historical evidence from food safety, financial markets,
            and aviation suggests that trust typically follows institutional
            frameworks‚Äînot voluntary disclosure alone.
          </p>
          <p class="about-text" style="margin-top: 1rem">
            The analysis does not claim that government regulation will
            automatically solve the trust gap. Rather, it documents that current
            voluntary approaches have not succeeded, creating a governance gap
            that justifies exploring institutional alternatives. The recommended
            path‚Äîco-regulation with third-party auditing‚Äîseeks to balance
            accountability with adaptability, drawing on the financial auditing
            model as a practical precedent.
          </p>
          <p class="about-text" style="margin-top: 1rem">
            <strong>Author:</strong> Sixing Tao<br />
            <strong>Institution:</strong> University of Washington
          </p>
        </div>
      </div>
    </section>

    <!-- ========================================================================
       Footer
       ======================================================================== -->
    <footer class="footer">
      <div class="container">
        <p>
          ¬© 2025 Sixing Tao ¬∑ University of Washington ¬∑
          <a href="#hero">Back to top</a>
        </p>
      </div>
    </footer>
  </body>
</html>
